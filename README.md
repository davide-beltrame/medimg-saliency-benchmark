# CNN Saliency Evaluation for Medical Imaging

Standard saliency methods such as Grad-CAM provide visual explanations for CNN predictions by highlighting input regions considered important. In this project, we use saliency methods to evaluate the explainability of deep learning models in predicting COVID-19 infection from [chest x-ray scans](https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia). Our objective is to assess how different networks rely on various image features for prediction, and how these differ from those identified by human experts. To this end, we collect manual annotations from a panel of medical professionals and compare them with Grad-CAM maps generated by the models. We aim to investigate the alignment between model- and expert-identified features, and analyze how this alignment relates to model performance and prediction accuracy.

The train set we used comprises images $I$ of chest x-ray scans, annotated with binary labels $y\in\{0,1\}$ indicating whether the scan is taken from a patient with covid 19 infection.

We implement and train 5 models (with appropriate GAP + Linear layers at the end to compute Grad-CAM):
- ResNet-50 (Strong baseline with residual connections; widely used in medical imaging, Good balance of performance and interpretability)
- DenseNet-121 (Proven effectiveness in chest X-ray tasks, Feature reuse via dense connections improves saliency sharpness)
- EfficientNet-B0 (High performance per parameter; well-suited for medical imaging with limited data)
- MobileNetV3 (Strong lightweight option for efficiency comparison)
optional (after we have completed the project):
- Vision Transformer (ViT-B/16)   - Offers fundamentally different saliency patterns, with attention-based reasoning)

We evaluate model performance on the entire test set using the metrics:
- Area Under the ROC Curve (x-axis = FP, y-axis = TP)
- F1 Score
- Accuracy
- Recall
- Precision

We sample 50 images from the test set at random and proceed with the "humna expert alignment" analysis.

We ask human experts to annotate the regions they deem more important to determine whether or not the patient has covid infection, we obtain a binary mask.

We validate the agreement between annotators by computing IoU.

We compute a final expert-level mask as the intersection of the annotated binary masks of all the experts, after processing with morphological binary filters.

We compute grad cams and compare them with the expert annotation using:
- IoU (on binarized grad CAMs vs binary annotations)
- Pointing Game (whether the highest activated region in the explanation map lies inside the expert-annotated region)

Finally, we study correlation between model performance and alignment metrics.

## Hyper-parameters
- Image input size = 224x224 (downsample test and train)

## Usage

### 1. Installation
Install the necessary Python packages using `pip`:

```bash
pip install -r requirements.txt
```

### 2. Data Preparation
First, download the dataset using the provided script. This requires having your Kaggle credentials set up correctly (`~/.kaggle/kaggle.json`).

```bash
python scripts/download_data.py
```
This will download the dataset to the path specified within the script (e.g., `data/raw/`).


## Repository Structure (Planned)
```bash
├── checkpoints/            
├── data/
│   ├── raw/                # (ignored by git)
│   ├── annotation_subset/  # Subset of images for manual annotation
│   └── annotations.csv                    
├── results/
│   ├── saliency_maps/      # Example saved saliency map visualizations
│   └── saliency_evaluation_summary.csv
├── scripts/
│   ├── download_data.py    
│   └── annotate_images.py  
├── src/
│   ├── __init__.py
│   ├── config.json
│   ├── datamodule.py       
│   ├── evaluate.py         
│   ├── models.py           
│   ├── train.py            
│   └── utils.py            
├── notebooks/              # Jupyter notebooks for exploration, visualization (optional)
├── .gitignore           
├── LICENSE                 
├── README.md               
└── requirements.txt        
```

## References (Preliminary)

* **Dataset:** Prashant Patel (2021). _Chest X-Ray (Covid-19 & Pneumonia)_. Kaggle. [https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia](https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia) (Acknowledges original sources within)
* **Grad-CAM:** Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). _Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization_. ICCV. ([arXiv:1610.02391](https://arxiv.org/abs/1610.02391))
* **(Models - Examples)**
    * He, K., Zhang, X., Ren, S., & Sun, J. (2016). _Deep Residual Learning for Image Recognition_. CVPR. ([arXiv:1512.03385](https://arxiv.org/abs/1512.03385))
    * Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). _Densely Connected Convolutional Networks_. CVPR. ([arXiv:1608.06993](https://arxiv.org/abs/1608.06993))
* **(Benchmarking Saliency - Example)** Saporta, A., et al. (2022). _Benchmarking saliency methods for chest X-ray interpretation_. Nature Machine Intelligence. ([DOI](https://doi.org/10.1038/s42256-022-00536-x)) - *For methodology inspiration.*