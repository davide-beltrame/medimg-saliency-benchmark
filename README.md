# Do AI and Human Experts See Pneumonia the Same Way?

Standard saliency methods such as Grad-CAM provide visual explanations for CNN predictions by highlighting input regions considered important. In this project, we use saliency methods to evaluate the explainability of deep learning models in predicting pneumonia from [chest x-ray scans](https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia). 

Our objective is to assess how different networks rely on various image features for prediction, and how these differ from those identified by human experts. To this end, we collect manual annotations from a panel of medical students and professionals and compare them with Grad-CAM maps generated by the models. We aim to investigate the alignment between model- and expert-identified features, and analyze how this alignment relates to model performance and prediction accuracy.

The train set we used comprises images $I$ of chest x-ray scans, annotated with binary labels $y\in\{0,1\}$ indicating whether the scan is taken from a patient with covid 19 infection.

We implement and train 5 models (with appropriate GAP + Linear layers at the end to compute Grad-CAM):
- AlexNet (Simple and efficient baseline architecture)
- VGG16 (Deep architecture with uniform 3x3 convolutions)
- InceptionNetV1 (Network with inception modules for multi-scale feature extraction)
- Resnet101 (Deep residual network with skip connections)

We evaluate model performance on the entire test set using the metrics:
- Area Under the ROC Curve (x-axis = FP, y-axis = TP)
- F1 Score
- Accuracy
- Recall
- Precision

We sample 50 images from the test set at random and proceed with the "human expert alignment" analysis.

We ask human experts to annotate the regions they deem more important to determine whether or not the patient has pneumonia, we obtain a binary mask.

We validate the agreement between annotators by computing IoU.

We compute a final expert-level mask as the intersection of the annotated binary masks of all the experts, after processing with morphological binary filters.

We compute grad cams and compare them with the expert annotation using:
- IoU (on binarized grad CAMs vs binary annotations)
- Pointing Game (whether the highest activated region in the explanation map lies inside the expert-annotated region)

Finally, we study correlation between model performance and alignment metrics.

## Expert Annotation Tool for Medical Imaging

To facilitate the collection of expert annotations, we developed a custom web-based tool (https://gciro.pythonanywhere.com) for medical professionals to identify relevant regions in chest X-ray images. 
The tool provides a simple drawing interface with adjustable brush sizes, allowing experts to highlight regions they consider diagnostically significant across 50 randomly selected chest X-rays of patients with pneumonia. The system automatically saves annotations and collects metadata including the participant's name and professional background.

Our panel of human experts comprises medical students, graduate students specializing in pneumology and radiology, and practicing medical professionals.

[TBD]
The collected annotations are analyzed by calculating inter-rater agreement using IoU between experts. We process these annotations using morphological filters and combine them to create consensus expert masks, which are then compared against model-generated Grad-CAM outputs. Through correlation analysis between model-expert alignment and predictive performance, we gain insights into the degree of alignment between AI vision systems and clinical expertise in pneumonia diagnosis, with important implications for explainable AI in medical imaging applications.

## Hyper-parameters
- Image input size = 224x224 (downsample test and train)

## Usage
Specify in the `config.json` the hyper-parameters for training:
```json
{
    "model": "an",          // ResNet-50 (rn50), DenseNet-121 (dn121), EfficientNet-B0 (eb0), MobileNetV3 (mv3), Vision Transformer (vit)
    "pretrained": false,    // Whether to load default weights
    "linear": false,        // Whether to override the original classifier and make it GAP + FC (does not work for rs and in, which are alredy GAP + FC)
    "batch_size": 32,       // train / val / test batch size
    "epochs": 1,            // number of training epochs
    "max_lr": 0.001,        // max lr in the OneCycle lr schedule
    "pct_start": 0.1,       // percentage of total steps for lr warmup
    "wandb":false           // log to WandB
}
```

Train a model on `data/train/`:

```
$ python train.py path/to/config.json
```

Test all the models the `checkpoints/` directory (config is required only for batch size):

```
$ python test.py path/to/config.json
```

### 1. Installation
Install the necessary Python packages using `pip`:

```bash
pip install -r requirements.txt
```

### 2. Data Preparation
First, download the dataset using the provided script. This requires having your Kaggle credentials set up correctly (`~/.kaggle/kaggle.json`).

```bash
python scripts/download_data.py
```
This will download the dataset to the path specified within the script (e.g., `data/raw/`).


## Repository Structure
```bash
├── LICENSE                 
├── README.md               
├── requirements.txt        
└── medimg-saliency-benchmark/     
    ├── data/                  
    │   ├── annotations/   
    │   │   ├── annotated/   
    │   │   └── original/     
    │   ├── test/               
    │   ├── train/              
    │   └── val/                
    ├── config.json                
    ├── datamodule.py   
    ├── eval_saliency_agreement.py
    ├── gridsearch.py        
    ├── main.ipynb              
    ├── models.py      
    ├── saliency.py   
    ├── test.py              
    ├── train.py              
    └── utils.py               
```

## References (Preliminary)

* **Dataset:** Prashant Patel (2021). _Chest X-Ray (Covid-19 & Pneumonia)_. Kaggle. [https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia](https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia) (Acknowledges original sources within)
* **Grad-CAM:** Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). _Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization_. ICCV. ([arXiv:1610.02391](https://arxiv.org/abs/1610.02391))
* **(Models - Examples)**
    * He, K., Zhang, X., Ren, S., & Sun, J. (2016). _Deep Residual Learning for Image Recognition_. CVPR. ([arXiv:1512.03385](https://arxiv.org/abs/1512.03385))
    * Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). _Densely Connected Convolutional Networks_. CVPR. ([arXiv:1608.06993](https://arxiv.org/abs/1608.06993))
* **(Benchmarking Saliency - Example)** Saporta, A., et al. (2022). _Benchmarking saliency methods for chest X-ray interpretation_. Nature Machine Intelligence. ([DOI](https://doi.org/10.1038/s42256-022-00536-x)) - *For methodology inspiration.*

## Acknowledgments

We would like to thank our annotators:

- **Tommaso Warner Danielli**, Ospedale Sant'Andrea, Sapienza University, Rome.
- etc.