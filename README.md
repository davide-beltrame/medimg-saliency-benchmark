# CNN Saliency Evaluation for Medical Imaging

Standard saliency methods such as Grad-CAM provide visual explanations for CNN predictions by highlighting input regions considered important. In this project, we use saliency methods to evaluate the explainability of deep learning models in predicting COVID-19 infection from [chest x-ray scans](https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia). Our objective is to assess how different networks rely on various image features for prediction, and how these differ from those identified by human experts. To this end, we collect manual annotations from a panel of medical professionals and compare them with Grad-CAM maps generated by the models. We aim to investigate the alignment between model- and expert-identified features, and analyze how this alignment relates to model performance and prediction accuracy.

The train set we used comprises images $I$ of chest x-ray scans, annotated with binary labels $y\in\{0,1\}$ indicating whether the scan is taken from a patient with covid 19 infection.

We implement and train 5 models (with appropriate GAP + Linear layers at the end to compute Grad-CAM):
- ResNet-50 (Strong baseline with residual connections; widely used in medical imaging, Good balance of performance and interpretability)
- DenseNet-121 (Proven effectiveness in chest X-ray tasks, Feature reuse via dense connections improves saliency sharpness)
- EfficientNet-B0 (High performance per parameter; well-suited for medical imaging with limited data)
- MobileNetV3 (Strong lightweight option for efficiency comparison)
- Vision Transformer (ViT-B/16)   - Offers fundamentally different saliency patterns, with attention-based reasoning)

We sample 50 images from the test set and evaluate model performance using:
- Area Under the ROC Curve (x-axis = FP, y-axis = TP)
- F1 Score
- Accuracy
- Recall
- Precision

We ask human experts to annotate the regions they deem more important to determine whether or not the patient has covid infection, we obtain a binary mask. 

We validate the agreement between annotators by computing IoU.

We compute a final expert-level mask as the intersection of the annotated binary masks of all the experts.

We compute grad cams and compare them with the expert annotation using:
- IoU (on binarized grad CAMs vs binary annotations)
- Pointing Game (whether the highest activated region in the explanation map lies inside the expert-annotated region)

Finally, we study correlation between model performance and alignment metrics.


## TO-DOs
- [x] Set up initial project structure and repository.
- [ ] Finalize `config.json` structure and parameters.
- [x] Implement data download script (`scripts/download_data.py`).
- [ ] Run data download script to obtain the dataset.
- [ ] Implement annotation script (`scripts/annotate_images.py`).
- [ ] Select ~50 test images (COVID/Normal) for the annotation subset.
- [ ] Perform manual annotation using the script and consolidate results (`data/annotations.csv`).
- [ ] Implement utility function to convert annotations (clicks/boxes) to masks.
- [ ] Implement data loaders (`src/datamodule.py`) for binary classification (COVID vs. Normal) with augmentation.
- [ ] Implement model building functions (`src/models.py`) for ResNet50, DenseNet121, VGG16, MobileNetV2 using transfer learning.
- [ ] Implement training script (`src/train.py`) with fine-tuning strategy and callbacks.
- [ ] Train all specified models and save best checkpoints.
- [ ] Implement Grad-CAM generation using `tf-keras-vis` (or `captum`).
- [ ] Implement evaluation script (`src/evaluate.py`) calculating IoU and Pointing Game metrics against annotation masks.
- [ ] Run evaluation script to generate results.
- [ ] Draft the project report using the provided LaTeX template.
- [ ] Complete helper functions in `src/utils.py`.
- [ ] Refine README and add visualizations/results summary.

## Key Decisions / Next Steps
- Verify data download and structure.
- Coordinate manual annotation process among team members.
- Finalize the exact set of hyperparameters for training in `config.json`.
- Decide on the specific thresholding strategy for Grad-CAM maps for IoU calculation.

## Usage

### 1. Installation
Install the necessary Python packages using `pip`:

```bash
pip install -r requirements.txt
```

### 2. Data Preparation
First, download the dataset using the provided script. This requires having your Kaggle credentials set up correctly (`~/.kaggle/kaggle.json`).

```bash
python scripts/download_data.py
```
This will download the dataset to the path specified within the script (e.g., `data/raw/`).


## Misc
1.  Create a `local_config.json` (added to `.gitignore`) for local testing without modifying the main `config.json`.
2.  If using Weights & Biases (`wandb`), log in via the CLI:
    ```bash
    python -c "import wandb; wandb.login()"
    ```

## Repository Structure (Planned)
```bash
├── checkpoints/            
├── data/
│   ├── raw/                # (ignored by git)
│   ├── annotation_subset/  # Subset of images for manual annotation
│   └── annotations.csv                    
├── results/
│   ├── saliency_maps/      # Example saved saliency map visualizations
│   └── saliency_evaluation_summary.csv
├── scripts/
│   ├── download_data.py    
│   └── annotate_images.py  
├── src/
│   ├── __init__.py
│   ├── config.json
│   ├── datamodule.py       
│   ├── evaluate.py         
│   ├── models.py           
│   ├── train.py            
│   └── utils.py            
├── notebooks/              # Jupyter notebooks for exploration, visualization (optional)
├── .gitignore           
├── LICENSE                 
├── README.md               
└── requirements.txt        
```

## References (Preliminary)

* **Dataset:** Prashant Patel (2021). _Chest X-Ray (Covid-19 & Pneumonia)_. Kaggle. [https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia](https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia) (Acknowledges original sources within)
* **Grad-CAM:** Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). _Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization_. ICCV. ([arXiv:1610.02391](https://arxiv.org/abs/1610.02391))
* **(Models - Examples)**
    * He, K., Zhang, X., Ren, S., & Sun, J. (2016). _Deep Residual Learning for Image Recognition_. CVPR. ([arXiv:1512.03385](https://arxiv.org/abs/1512.03385))
    * Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). _Densely Connected Convolutional Networks_. CVPR. ([arXiv:1608.06993](https://arxiv.org/abs/1608.06993))
* **(Benchmarking Saliency - Example)** Saporta, A., et al. (2022). _Benchmarking saliency methods for chest X-ray interpretation_. Nature Machine Intelligence. ([DOI](https://doi.org/10.1038/s42256-022-00536-x)) - *For methodology inspiration.*